### Node-Mongo 

使用Nodejs配合Mongodb做数据批量插入测试，数据是5000万条出租车数据。
为了减少数据库的空间占用，键值尽量使用1个字母，对应表如下：
* l 出租车的经纬度数据，数字数组
* t 传感器记录时间，标准时间格式
* i 是否有乘客，布尔值
* n 出租车牌号，含有中文的字符串
* d 当前方向
* v 当前速度

开始的插入方式是遍历目标路径，获取所有二进制文件的路径数组，异步读取所有二进制文件，获取记录数组，而后异步插入。
bulkInsert中记录了批量插入的写法，并且注释了小批量数据插入的测试代码，暂时没有满意的效果。

主要的问题在于：
* mongodb 插入有单次最大一千条的限制
* mongodb 驱动的bulk insert api性能提升和单次循环插入相比不明显
* bulk 操作在全为同一种操作（比如插入）时，顺序执行反而比乱序执行快，但官方文档认为乱序是效率更高的方式

-------

####7.21

目前使用slicedInsert.js可以有最快的效率，每100个二进制文件做为一个串行化单位。
使用实验室的集群Master主机可以达到4.2万条/秒的插入速度（3700万条插入的平均时间）
单维度索引建立时间为9-13分钟（针对1.124亿条数据的操作）
同等条件查询在有索引后速度提升明显。

####7.22

尝试async模块的cargo做插入的负载均衡

